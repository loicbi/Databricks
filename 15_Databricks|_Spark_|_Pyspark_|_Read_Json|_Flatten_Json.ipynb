{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1rOlnpxy_p3aDFuHKpjRlBI7h9M-NwkqV",
      "authorship_tag": "ABX9TyOvqMAHMK4f1mudCwJTWTfx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/loicbi/Databricks/blob/develop/15_Databricks%7C_Spark_%7C_Pyspark_%7C_Read_Json%7C_Flatten_Json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5poKsdENgL1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "PAHeaUwSIS1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3N_gZyPFhMw"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "from pyspark.sql.functions import  * # to_date, col, split, char_length, lit, trim\n",
        "# findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "# spark = SparkSession.builder.appName(\"app_name\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "\n",
        "\n",
        "# df = spark.read.option('multiline', 'true').json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/US_category_id.json\")\n",
        "df = spark.read.json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/US_category_id.json\", multiLine=True)\n",
        "# df = spark.read.json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/product.json\", multiLine=True)\n",
        "df.printSchema()\n",
        "df.show(10)\n"
      ],
      "metadata": {
        "id": "2YNmbGsDFon9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Flatten array of structs and structs\n",
        "def flatten_df(df):\n",
        "  #  compute Complex Fields (Lists and Structs) in Schema\n",
        "   complex_fields = dict([(field.name, field.dataType)\n",
        "                             for field in df.schema.fields\n",
        "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
        "   while len(complex_fields)!=0:\n",
        "      col_name=list(complex_fields.keys())[0]\n",
        "      print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
        "\n",
        "      # if StructType then convert all sub element to columns.\n",
        "      # i.e. flatten structs\n",
        "      if (type(complex_fields[col_name]) == StructType):\n",
        "         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
        "         df=df.select(\"*\", *expanded).drop(col_name)\n",
        "\n",
        "      # if ArrayType then add the Array Elements as Rows using the explode function\n",
        "      # i.e. explode Arrays\n",
        "      elif (type(complex_fields[col_name]) == ArrayType):\n",
        "         df=df.withColumn(col_name,explode_outer(col_name))\n",
        "\n",
        "      # recompute remaining Complex Fields in Schema\n",
        "      complex_fields = dict([(field.name, field.dataType)\n",
        "                             for field in df.schema.fields\n",
        "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
        "   return df"
      ],
      "metadata": {
        "id": "AQhewVetmaG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_flatten = flatten_df(df)"
      ],
      "metadata": {
        "id": "pYv1Tt87mpFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_flatten.show(5)"
      ],
      "metadata": {
        "id": "Qu741ttJqSOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## json from link"
      ],
      "metadata": {
        "id": "Ow6igr1bn-VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "import findspark\n",
        "import json\n",
        "from pyspark.sql.functions import  * # to_date, col, split, char_length, lit, trim\n",
        "# findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "# spark = SparkSession.builder.appName(\"app_name\").getOrCreate()\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "\n",
        "\n",
        "#Flatten array of structs and structs\n",
        "def flatten_df_all(df):\n",
        "   i_column_exist = 0\n",
        "  #  compute Complex Fields (Lists and Structs) in Schema\n",
        "   complex_fields = dict([(field.name, field.dataType)\n",
        "                             for field in df.schema.fields\n",
        "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
        "   while len(complex_fields)!=0:\n",
        "      col_name=list(complex_fields.keys())[0]\n",
        "      # print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
        "\n",
        "      # if StructType then convert all sub element to columns.\n",
        "      # i.e. flatten structs\n",
        "      if (type(complex_fields[col_name]) == StructType):\n",
        "         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
        "         df=df.select(\"*\", *expanded).drop(col_name)\n",
        "\n",
        "      # if ArrayType then add the Array Elements as Rows using the explode function\n",
        "      # i.e. explode Arrays\n",
        "      elif (type(complex_fields[col_name]) == ArrayType):\n",
        "          # check if colNameList [] contains StructType ==> [{}]\n",
        "          print(isinstance(df.schema[col_name].dataType.elementType, StructType))\n",
        "          df.select(col_name).show()\n",
        "          # if isinstance(df.schema[col_name].dataType.elementType, StructType) is True:\n",
        "          if col_name in df.columns:\n",
        "            df=df.withColumnRenamed(col_name,col_name+'_'+str(i_column_exist)).withColumn(col_name+'_'+str(i_column_exist),explode_outer(col_name+'_'+str(i_column_exist)))\n",
        "          else:\n",
        "            df=df.withColumn(col_name,explode_outer(col_name))\n",
        "\n",
        "\n",
        "      # recompute remaining Complex Fields in Schema\n",
        "      complex_fields = dict([(field.name, field.dataType)\n",
        "                             for field in df.schema.fields\n",
        "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
        "   i_column_exist += 1\n",
        "   return df\n",
        "\n",
        "# method from link\n",
        "# url = 'https://world.openfoodfacts.org/api/v2/product/737628064502.json'\n",
        "# jsonData = urlopen(url).read().decode('utf-8')\n",
        "# rdd = spark.sparkContext.parallelize([jsonData])\n",
        "# df_link = spark.read.json(rdd, multiLine=True)\n",
        "\n",
        "# method from drive\n",
        "df_product_2 = spark.read.json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/product2.json\", multiLine=True)\n",
        "\n",
        "# Create outer method to return the flattened Data Frame\n",
        "def flatten_json_df(_df: DataFrame) -> DataFrame:\n",
        "    # List to hold the dynamically generated column names\n",
        "    flattened_col_list = []\n",
        "\n",
        "    # Inner method to iterate over Data Frame to generate the column list\n",
        "    def get_flattened_cols(df: DataFrame, struct_col: str = None) -> None:\n",
        "        for col in df.columns:\n",
        "            # print(col, df.schema[col].dataType.typeName(), df.schema[col].dataType)\n",
        "\n",
        "            if df.schema[col].dataType.typeName() != 'struct':\n",
        "                if (isinstance(df.schema[col].dataType, ArrayType)):\n",
        "                  if (isinstance(df.schema[col].dataType.elementType, StructType)):\n",
        "                    array_struct_col = col\n",
        "                    # print(col, 'struct_col:', struct_col)\n",
        "\n",
        "                    for col_name in (json.loads(df.schema[col].dataType.json())['elementType']['fields']):\n",
        "                        # for c in json.loads(df.schema[col].dataType.json())['elementType']['fields'][n]['name']:\n",
        "                        # print(col_name['name'])\n",
        "                        col_child = col_name['name']\n",
        "                        array_chained_col = array_struct_col +\".\"+ col_child if array_struct_col is not None else col\n",
        "                        # get_flattened_cols(df.select(col_child+\".*\"), col)\n",
        "                        print(array_chained_col)\n",
        "\n",
        "\n",
        "\n",
        "                        # array_t = struct_col + \".\" + col\n",
        "                        # flattened_col_list.append(f\"{t} as {t.replace('.','_')}\")\n",
        "\n",
        "\n",
        "\n",
        "                    # if ('type' in (json.loads(df.schema[col].dataType.json()))):\n",
        "                    #   print(col, 'contains structure')\n",
        "                    # else:\n",
        "                    #   print(col, 'doesn\\'t contains structure')\n",
        "\n",
        "\n",
        "\n",
        "                if struct_col is None:\n",
        "                    flattened_col_list.append(f\"{col} as {col.replace('.','_')}\")\n",
        "                else:\n",
        "                    t = struct_col + \".\" + col\n",
        "                    flattened_col_list.append(f\"{t} as {t.replace('.','_')}\")\n",
        "                    print('not struc', t)\n",
        "            else:\n",
        "                chained_col = struct_col +\".\"+ col if struct_col is not None else col\n",
        "                print('struct', chained_col)\n",
        "                get_flattened_cols(df.select(col+\".*\"), chained_col)\n",
        "\n",
        "    # Call the inner Method\n",
        "    get_flattened_cols(_df)\n",
        "\n",
        "    # Return the flattened Data Frame\n",
        "    return _df.selectExpr(flattened_col_list)# Generate the flattened DF\n",
        "\n",
        "# df.printSchema()\n",
        "df_link_flatten = flatten_json_df(df_product_2)\n",
        "df_link_flatten.show()\n",
        "# df_link = spark.read.json('https://world.openfoodfacts.org/api/v2/product/737628064502.json')"
      ],
      "metadata": {
        "id": "E3_9m_tAn9VX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "data = r'''\n",
        "{\n",
        " \"kind\": \"youtube#videoCategoryListResponse\",\n",
        " \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJvJAAShlR6hM\\\"\",\n",
        " \"items\": [\n",
        "  {\n",
        "   \"kind\": \"youtube#videoCategory\",\n",
        "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/Xy1mB4_yLrHy_BmKmPBggty2mZQ\\\"\",\n",
        "   \"id\": \"1\",\n",
        "   \"snippet\": {\n",
        "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
        "    \"title\": \"Film & Animation\",\n",
        "    \"assignable\": true\n",
        "   }\n",
        "  },\n",
        "  {\n",
        "   \"kind\": \"youtube#videoCategory\",\n",
        "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\\\"\",\n",
        "   \"id\": \"2\",\n",
        "   \"snippet\": {\n",
        "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
        "    \"title\": \"Autos & Vehicles\",\n",
        "    \"assignable\": true\n",
        "   }\n",
        "  },\n",
        "  {\n",
        "   \"kind\": \"youtube#videoCategory\",\n",
        "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/nqRIq97-xe5XRZTxbknKFVe5Lmg\\\"\",\n",
        "   \"id\": \"10\",\n",
        "   \"snippet\": {\n",
        "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
        "    \"title\": \"Music\",\n",
        "    \"assignable\": true\n",
        "   }\n",
        "  },\n",
        "  {\n",
        "   \"kind\": \"youtube#videoCategory\",\n",
        "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/HwXKamM1Q20q9BN-oBJavSGkfDI\\\"\",\n",
        "   \"id\": \"15\",\n",
        "   \"snippet\": {\n",
        "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
        "    \"title\": \"Pets & Animals\",\n",
        "    \"assignable\": true\n",
        "   }\n",
        "  },\n",
        "  {\n",
        "   \"kind\": \"youtube#videoCategory\",\n",
        "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/9GQMSRjrZdHeb1OEM1XVQ9zbGec\\\"\",\n",
        "   \"id\": \"17\",\n",
        "   \"snippet\": {\n",
        "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
        "    \"title\": \"Sports\",\n",
        "    \"assignable\": true\n",
        "   }\n",
        "  },\n",
        "  {\n",
        "   \"kind\": \"youtube#videoCategory\",\n",
        "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/FJwVpGCVZ1yiJrqZbpqe68Sy_OE\\\"\",\n",
        "   \"id\": \"18\",\n",
        "   \"snippet\": {\n",
        "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
        "    \"title\": \"Short Movies\",\n",
        "    \"assignable\": false\n",
        "   }\n",
        "  },\n",
        "  {\n",
        "   \"kind\": \"youtube#videoCategory\",\n",
        "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/M-3iD9dwK7YJCafRf_DkLN8CouA\\\"\",\n",
        "   \"id\": \"19\",\n",
        "   \"snippet\": {\n",
        "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
        "    \"title\": \"Travel & Events\",\n",
        "    \"assignable\": true\n",
        "   }\n",
        "  }\n",
        " ]\n",
        "} '''\n",
        "data = json.loads(data)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "3dUg6JAEqUOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark — Flatten JSON/Struct Data Frame dynamically\n",
        "### Example variable local\n",
        "\n",
        "[JSON/Struct Data Frame dynamically](https://subhamkharwal.medium.com/pyspark-flatten-json-struct-data-frame-dynamically-c2e5d8937dcc)"
      ],
      "metadata": {
        "id": "q2EBMvkP6Vsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create an Example Data Frame to hold JSON data\n",
        "# Example Data Frame with column having JSON data\n",
        "_data = [\n",
        "    ['EMP001', '{\"dept\" : \"account\", \"fname\": \"Ramesh\", \"lname\": \"Singh\", \"skills\": [\"excel\", \"tally\", \"word\"]}'],\n",
        "    ['EMP002', '{\"dept\" : \"sales\", \"fname\": \"Siv\", \"lname\": \"Kumar\", \"skills\": [\"biking\", \"sales\"]}'],\n",
        "    ['EMP003', '{\"dept\" : \"hr\", \"fname\": \"MS Raghvan\", \"skills\": [\"communication\", \"soft-skills\"], \"hobbies\" : {\"cycling\": \"expert\", \"computers\":\"basic\"}}'],\n",
        "    ['EMP003', '{\"dept\" : \"hr\", \"fname\": \"MS Raghvan\", \"skills\": [\"communication\", \"soft-skills\"], \"hobbies\" : {\"cycling\": \"expert\", \"computers\":\"basic\"}}']\n",
        "]\n",
        "\n",
        "# Columns for the data\n",
        "_cols = ['emp_no', 'raw_data']\n",
        "\n",
        "# Lets create the raw Data Frame\n",
        "df_raw = spark.createDataFrame(data = _data, schema = _cols)\n",
        "\n",
        "# Determine the schema of the JSON payload from the column\n",
        "json_schema_df = spark.read.json(df_raw.rdd.map(lambda row: row.raw_data))\n",
        "json_schema = json_schema_df.schema\n",
        "\n",
        "# Apply the schema to payload to read the data\n",
        "from pyspark.sql.functions import from_json\n",
        "df_details = df_raw.withColumn(\"emp_details\", from_json(df_raw[\"raw_data\"], json_schema)).drop(\"raw_data\")\n",
        "df_details.show(10, False)\n",
        "\n",
        "df_details.printSchema()"
      ],
      "metadata": {
        "id": "jQjICq4g4_G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python function to flatten the data dynamically\n",
        "from pyspark.sql import DataFrame\n",
        "# Create outer method to return the flattened Data Frame\n",
        "def flatten_json_df(_df: DataFrame) -> DataFrame:\n",
        "    # List to hold the dynamically generated column names\n",
        "    flattened_col_list = []\n",
        "\n",
        "    # Inner method to iterate over Data Frame to generate the column list\n",
        "    def get_flattened_cols(df: DataFrame, struct_col: str = None) -> None:\n",
        "        for col in df.columns:\n",
        "            if df.schema[col].dataType.typeName() != 'struct':\n",
        "                if struct_col is None:\n",
        "                    flattened_col_list.append(f\"{col} as {col.replace('.','_')}\")\n",
        "                else:\n",
        "                    t = struct_col + \".\" + col\n",
        "                    flattened_col_list.append(f\"{t} as {t.replace('.','_')}\")\n",
        "            else:\n",
        "                chained_col = struct_col +\".\"+ col if struct_col is not None else col\n",
        "                get_flattened_cols(df.select(col+\".*\"), chained_col)\n",
        "\n",
        "    # Call the inner Method\n",
        "    get_flattened_cols(_df)\n",
        "\n",
        "    # Return the flattened Data Frame\n",
        "    return _df.selectExpr(flattened_col_list)# Generate the flattened DF\n",
        "flattened_df = flatten_json_df(df_details)\n",
        "flattened_df.show(10)\n",
        "# Print Schema\n",
        "flattened_df.printSchema()"
      ],
      "metadata": {
        "id": "7X0IutCLHTKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "data = '''\n",
        "{\"customer_id\":45721,\"firstname\":\"Tiffany\",\"lastname\":\"Skinner\",\"salutation\":\"Miss\",\"gender\":\"F\",\"birthdate\":\"1935-09-13\",\"birth_country\":\"RWANDA\",\"address_id\":2133,\"email_address\":\"Tiffany.Skinner@J.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[50001,60000],\"buy_potential\":\">10000\",\"purchase_estimate\":1000,\"vehicle_count\":2,\"credit_rating\":\"High Risk\",\"education_status\":\"Secondary\"}}\n",
        "{\"customer_id\":72411,\"firstname\":\"Dennis\",\"lastname\":\"Pettit\",\"salutation\":\"Mr.\",\"gender\":\"F\",\"birthdate\":\"1973-11-09\",\"birth_country\":\"MALAWI\",\"address_id\":25398,\"email_address\":\"Dennis.Pettit@PtS7Fbdax.com\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[170001,180000],\"buy_potential\":\"0-500\",\"purchase_estimate\":3000,\"vehicle_count\":4,\"credit_rating\":\"Unknown\",\"education_status\":\"Primary\"}}\n",
        "{\"customer_id\":50358,\"firstname\":\"Issac\",\"lastname\":\"Saunders\",\"salutation\":\"Mr.\",\"gender\":\"F\",\"birthdate\":\"1979-03-30\",\"birth_country\":\"JAMAICA\",\"address_id\":23242,\"email_address\":\"Issac.Saunders@Ykkz.edu\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[120001,130000],\"buy_potential\":\"0-500\",\"purchase_estimate\":2500,\"vehicle_count\":3,\"credit_rating\":\"Unknown\",\"education_status\":\"Unknown\"}}\n",
        "{\"customer_id\":96143,\"firstname\":\"Carl\",\"lastname\":\"Mitchell\",\"salutation\":\"Dr.\",\"gender\":\"F\",\"birthdate\":\"1966-10-16\",\"birth_country\":\"IRELAND\",\"address_id\":37550,\"email_address\":\"Carl.Mitchell@iqios5dGzoAkgSfdkO.org\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[20001,30000],\"buy_potential\":\"Unknown\",\"purchase_estimate\":1000,\"vehicle_count\":-1,\"credit_rating\":\"High Risk\",\"education_status\":\"College\"}}\n",
        "{\"customer_id\":93545,\"firstname\":\"Antonio\",\"lastname\":\"Lyon\",\"salutation\":\"Dr.\",\"gender\":\"F\",\"birthdate\":\"1953-05-24\",\"birth_country\":\"PAPUA NEW GUINEA\",\"address_id\":22369,\"email_address\":\"Antonio.Lyon@Alqrvh.org\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[180001,190000],\"buy_potential\":\"1001-5000\",\"purchase_estimate\":8000,\"vehicle_count\":1,\"credit_rating\":\"Unknown\",\"education_status\":\"4 yr Degree\"}}\n",
        "{\"customer_id\":92534,\"firstname\":\"Jesse\",\"lastname\":\"Alexander\",\"salutation\":\"Dr.\",\"gender\":\"M\",\"birthdate\":\"1968-10-22\",\"birth_country\":\"ZIMBABWE\",\"address_id\":7628,\"email_address\":\"Jesse.Alexander@OU.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[120001,130000],\"buy_potential\":\">10000\",\"purchase_estimate\":8000,\"vehicle_count\":2,\"credit_rating\":\"Unknown\",\"education_status\":\"Advanced Degree\"}}\n",
        "{\"customer_id\":91798,\"firstname\":\"Philip\",\"lastname\":\"Miller\",\"salutation\":\"Mr.\",\"gender\":\"M\",\"birthdate\":\"1934-08-05\",\"birth_country\":\"DJIBOUTI\",\"address_id\":24321,\"email_address\":\"Philip.Miller@1j1uLUoeT3N.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[150001,160000],\"buy_potential\":\"5001-10000\",\"purchase_estimate\":500,\"vehicle_count\":3,\"credit_rating\":\"High Risk\",\"education_status\":\"Unknown\"}}\n",
        "{\"customer_id\":82953,\"firstname\":\"Ronald\",\"lastname\":\"Harris\",\"salutation\":\"Sir\",\"gender\":\"M\",\"birthdate\":\"1958-08-02\",\"birth_country\":\"LESOTHO\",\"address_id\":49705,\"email_address\":\"Ronald.Harris@KUjp3Gqoqt.com\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[20001,30000],\"buy_potential\":\"501-1000\",\"purchase_estimate\":6000,\"vehicle_count\":4,\"credit_rating\":\"Good\",\"education_status\":\"4 yr Degree\"}}\n",
        "{\"customer_id\":70926,\"firstname\":\"Myrtle\",\"lastname\":\"Wolford\",\"salutation\":\"Dr.\",\"gender\":\"F\",\"birthdate\":\"1983-11-14\",\"birth_country\":\"C�TE D'IVOIRE\",\"address_id\":36381,\"email_address\":\"Myrtle.Wolford@lbKOsj.com\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[110001,120000],\"buy_potential\":\"Unknown\",\"purchase_estimate\":2500,\"vehicle_count\":0,\"credit_rating\":\"Unknown\",\"education_status\":\"2 yr Degree\"}}\n",
        "{\"customer_id\":12157,\"firstname\":\"William\",\"lastname\":\"Daniel\",\"salutation\":\"Mr.\",\"gender\":\"F\",\"birthdate\":\"1976-03-27\",\"birth_country\":\"BANGLADESH\",\"address_id\":40974,\"email_address\":\"William.Daniel@Ri4LeFs5lB4QpUfF.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[170001,180000],\"buy_potential\":\">10000\",\"purchase_estimate\":6000,\"vehicle_count\":2,\"credit_rating\":\"Unknown\",\"education_status\":\"Primary\"}}\n",
        "{\"customer_id\":65983,\"firstname\":\"Perla\",\"lastname\":\"Edwards\",\"salutation\":\"Dr.\",\"gender\":\"F\",\"birthdate\":\"1947-02-27\",\"birth_country\":\"LIBERIA\",\"address_id\":3282,\"email_address\":\"Perla.Edwards@nvR7A3h9o.edu\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[180001,190000],\"buy_potential\":\"0-500\",\"purchase_estimate\":9500,\"vehicle_count\":1,\"credit_rating\":\"Good\",\"education_status\":\"Advanced Degree\"}}\n",
        "{\"customer_id\":36493,\"firstname\":\"Jeffrey\",\"lastname\":\"Clary\",\"salutation\":\"Mr.\",\"gender\":\"F\",\"birthdate\":\"1924-02-17\",\"birth_country\":\"ROMANIA\",\"address_id\":1289,\"email_address\":\"Jeffrey.Clary@lBvUIdcnhNFey.com\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[40001,50000],\"buy_potential\":\"0-500\",\"purchase_estimate\":2500,\"vehicle_count\":1,\"credit_rating\":\"Low Risk\",\"education_status\":\"2 yr Degree\"}}\n",
        "{\"customer_id\":41148,\"firstname\":\"Charles\",\"lastname\":\"Patterson\",\"salutation\":\"Mr.\",\"gender\":\"M\",\"birthdate\":\"1961-08-12\",\"birth_country\":\"DOMINICA\",\"address_id\":29846,\"email_address\":\"Charles.Patterson@bHiA5JcaZzpS89L8iL.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[120001,130000],\"buy_potential\":\"0-500\",\"purchase_estimate\":10000,\"vehicle_count\":1,\"credit_rating\":\"Unknown\",\"education_status\":\"Advanced Degree\"}}\n",
        "{\"customer_id\":61969,\"firstname\":\"Nikki\",\"lastname\":\"Watson\",\"salutation\":\"Miss\",\"gender\":\"M\",\"birthdate\":\"1977-08-15\",\"birth_country\":\"PARAGUAY\",\"address_id\":22881,\"email_address\":\"Nikki.Watson@DNC.org\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[90001,100000],\"buy_potential\":\"1001-5000\",\"purchase_estimate\":6500,\"vehicle_count\":0,\"credit_rating\":\"High Risk\",\"education_status\":\"Advanced Degree\"}}\n",
        "{\"customer_id\":69480,\"firstname\":\"Marion\",\"lastname\":\"Paul\",\"salutation\":\"Sir\",\"gender\":\"M\",\"birthdate\":\"1955-10-11\",\"birth_country\":\"SAUDI ARABIA\",\"address_id\":42838,\"email_address\":\"Marion.Paul@tfyhY8prgDTSdZ.org\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[100001,110000],\"buy_potential\":\"5001-10000\",\"purchase_estimate\":3000,\"vehicle_count\":0,\"credit_rating\":\"Unknown\",\"education_status\":\"Unknown\"}}\n",
        "{\"customer_id\":4739,\"firstname\":\"Gregory\",\"lastname\":\"Martin\",\"salutation\":\"Mr.\",\"gender\":\"M\",\"birthdate\":\"1958-03-15\",\"birth_country\":\"UGANDA\",\"address_id\":2625,\"email_address\":\"Gregory.Martin@GI78ZP8x.edu\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[130001,140000],\"buy_potential\":\"1001-5000\",\"purchase_estimate\":5500,\"vehicle_count\":4,\"credit_rating\":\"Good\",\"education_status\":\"Unknown\"}}\n",
        "{\"customer_id\":67634,\"salutation\":\"Sir\",\"gender\":\"F\",\"address_id\":37149,\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[140001,150000],\"buy_potential\":\"Unknown\",\"purchase_estimate\":9000,\"vehicle_count\":1,\"credit_rating\":\"High Risk\",\"education_status\":\"Primary\"}}\n",
        "{\"customer_id\":87143,\"firstname\":\"Corazon\",\"lastname\":\"Martinez\",\"salutation\":\"Miss\",\"gender\":\"F\",\"birthdate\":\"1984-08-13\",\"birth_country\":\"SAMOA\",\"address_id\":27621,\"email_address\":\"Corazon.Martinez@BrDtDnXnkci1Pp2an.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[30001,40000],\"buy_potential\":\"5001-10000\",\"purchase_estimate\":9000,\"vehicle_count\":4,\"credit_rating\":\"High Risk\",\"education_status\":\"Unknown\"}}\n",
        "{\"customer_id\":64173,\"firstname\":\"Christopher\",\"lastname\":\"Byrd\",\"salutation\":\"Sir\",\"gender\":\"M\",\"birthdate\":\"1936-04-10\",\"birth_country\":\"MALDIVES\",\"address_id\":21565,\"email_address\":\"Christopher.Byrd@bLMxmInbyn.org\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[20001,30000],\"buy_potential\":\"0-500\",\"purchase_estimate\":8500,\"vehicle_count\":4,\"credit_rating\":\"Good\",\"education_status\":\"2 yr Degree\"}}\n",
        "{\"customer_id\":39412,\"firstname\":\"Constance\",\"lastname\":\"Seaman\",\"salutation\":\"Miss\",\"gender\":\"F\",\"birthdate\":\"1980-03-10\",\"birth_country\":\"AUSTRALIA\",\"address_id\":18881,\"email_address\":\"Constance.Seaman@GItsENvPOs7C.org\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[40001,50000],\"buy_potential\":\"5001-10000\",\"purchase_estimate\":3000,\"vehicle_count\":2,\"credit_rating\":\"High Risk\",\"education_status\":\"4 yr Degree\"}}\n",
        "{\"customer_id\":39412,\"firstname\":\"Constance\",\"lastname\":\"Seaman\",\"salutation\":\"Miss\",\"gender\":\"F\",\"birthdate\":\"1980-03-10\",\"birth_country\":\"AUSTRALIA\",\"address_id\":18881,\"email_address\":\"Constance.Seaman@GItsENvPOs7C.org\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[40001,50000],\"buy_potential\":\"5001-10000\",\"purchase_estimate\":3000,\"vehicle_count\":2,\"credit_rating\":\"High Risk\",\"education_status\":\"4 yr Degree\"}}\n",
        "'''\n",
        "\n",
        "import json\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def flatten_json(json_obj, prefix=''):\n",
        "    flattened = {}\n",
        "    for key, value in json_obj.items():\n",
        "        new_key = prefix + key\n",
        "        if isinstance(value, dict):\n",
        "            flattened.update(flatten_json(value, new_key + '_'))\n",
        "        elif isinstance(value, list):\n",
        "            for i, item in enumerate(value):\n",
        "                if isinstance(item, dict):\n",
        "                    flattened.update(flatten_json(item, new_key + f'_{i}_'))\n",
        "                else:\n",
        "                    flattened[new_key + f'_{i}'] = item\n",
        "        else:\n",
        "            flattened[new_key] = value\n",
        "    return flattened\n",
        "\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Load the data as separate JSON objects\n",
        "json_objects = [json.loads(line) for line in data.strip().split('\\n')]\n",
        "\n",
        "# Flatten the JSON objects\n",
        "flattened_objects = [flatten_json(obj) for obj in json_objects]\n",
        "\n",
        "# Convert the flattened objects into a Spark DataFrame\n",
        "df = spark.createDataFrame(flattened_objects)\n",
        "df.printSchema()\n",
        "# Show the flattened DataFrame\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "BAOJHPQTFXb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qcz0z46RQ2Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_customer =  spark.read.json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/customer.json\")\n",
        "df_customer.printSchema()\n",
        "# df_customer.show()\n",
        "# Flatten the JSON objects\n",
        "# flattened_objects = [flatten_json(obj) for obj in df_customer]\n",
        "\n",
        "# # Convert the flattened objects into a Spark DataFrame\n",
        "df = flatten_df(df_customer)\n",
        "\n",
        "df.orderBy(df.customer_id).show()\n",
        "df.printSchema()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Sh7xOEWhBxA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Re1G5IcHAPAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flattening deep nested JSON using python and Databricks\n",
        "[link text](https://sanajitghosh.medium.com/flattening-deep-nested-json-using-python-and-databricks-110d097efa69)"
      ],
      "metadata": {
        "id": "Vekd5Y6-Ns_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import arrays_zip\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "#load json object\n",
        "df = spark.read.option(\"multiline\",\"true\").json('/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/data.json')\n",
        "\n",
        "def child_struct(nested_df):\n",
        "    # Creating python list to store dataframe metadata\n",
        "    list_schema = [((), nested_df)]\n",
        "    # Creating empty python list for final flattern columns\n",
        "    flat_columns = []\n",
        "\n",
        "    while len(list_schema) > 0:\n",
        "      # Removing latest or recently added item (dataframe schema) and returning into df variable\n",
        "          parents, df = list_schema.pop()\n",
        "          flat_cols = [  col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],))) for c in df.dtypes if c[1][:6] != \"struct\"   ]\n",
        "\n",
        "          struct_cols = [  c[0]   for c in df.dtypes if c[1][:6] == \"struct\"   ]\n",
        "\n",
        "          flat_columns.extend(flat_cols)\n",
        "          #Reading  nested columns and appending into stack list\n",
        "          for i in struct_cols:\n",
        "                projected_df = df.select(i + \".*\")\n",
        "                list_schema.append((parents + (i,), projected_df))\n",
        "    return nested_df.select(flat_columns)\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "def master_array(df):\n",
        "    array_cols = [c[0] for c in df.dtypes if c[1][:5]==\"array\"]\n",
        "    while len(array_cols)>0:\n",
        "        # array\n",
        "        for c in array_cols:\n",
        "           df = df.withColumn(c,explode_outer(c))\n",
        "        df = child_struct(df)\n",
        "        array_cols = [c[0] for c in df.dtypes if c[1][:5]==\"array\"]\n",
        "    return df\n",
        "\n",
        "df_output = master_array(df)\n",
        "display(df_output)\n",
        "df_output.show(10)\n",
        "\n"
      ],
      "metadata": {
        "id": "lQqEIDYiNscF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "def flatten_structs(nested_df):\n",
        "    stack = [((), nested_df)]\n",
        "    columns = []\n",
        "\n",
        "    while len(stack) > 0:\n",
        "\n",
        "        parents, df = stack.pop()\n",
        "\n",
        "        array_cols = [\n",
        "            c[0]\n",
        "            for c in df.dtypes\n",
        "            if c[1][:5] == \"array\"\n",
        "        ]\n",
        "\n",
        "        flat_cols = [\n",
        "            f.col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\n",
        "            for c in df.dtypes\n",
        "            if c[1][:6] != \"struct\"\n",
        "        ]\n",
        "\n",
        "        nested_cols = [\n",
        "            c[0]\n",
        "            for c in df.dtypes\n",
        "            if c[1][:6] == \"struct\"\n",
        "        ]\n",
        "\n",
        "        columns.extend(flat_cols)\n",
        "\n",
        "        for nested_col in nested_cols:\n",
        "            projected_df = df.select(nested_col + \".*\")\n",
        "            stack.append((parents + (nested_col,), projected_df))\n",
        "\n",
        "    return nested_df.select(columns)\n",
        "\n",
        "def flatten_array_struct_df(df):\n",
        "\n",
        "    array_cols = [\n",
        "            c[0]\n",
        "            for c in df.dtypes\n",
        "            if c[1][:5] == \"array\"\n",
        "        ]\n",
        "\n",
        "\n",
        "    while len(array_cols) > 0:\n",
        "\n",
        "        for array_col in array_cols:\n",
        "            if isinstance(df.schema[array_col].dataType.elementType, StructType) is True:\n",
        "\n",
        "                cols_to_select = [x for x in df.columns if x != array_col ]\n",
        "\n",
        "                df = df.withColumn(array_col, f.explode(f.col(array_col)))\n",
        "\n",
        "        df = flatten_structs(df)\n",
        "\n",
        "        array_cols = [\n",
        "            c[0]\n",
        "            for c in df.dtypes\n",
        "            if c[1][:5] == \"array\"\n",
        "        ]\n",
        "        return df\n",
        "\n",
        "flat_df = flatten_array_struct_df(df_product_2)\n",
        "\n",
        "flat_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4brJ8hec382b",
        "outputId": "3f457344-eb0c-4ad1-e449-ac7836a838e7"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+---------------+---------------------+------------------------+-------------+---------+\n",
            "|customer_id|customer_name|customer_number|customer_address_city|customer_address_country|orders_amount|orders_id|\n",
            "+-----------+-------------+---------------+---------------------+------------------------+-------------+---------+\n",
            "|        123|     John Doe|[1, 2, 3, 4, 5]|             New York|                     USA|           50|      101|\n",
            "|        123|     John Doe|[1, 2, 3, 4, 5]|             New York|                     USA|          100|      102|\n",
            "+-----------+-------------+---------------+---------------------+------------------------+-------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## David et Moi"
      ],
      "metadata": {
        "id": "yztcvZaZ7HM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as f\n",
        "from urllib.request import urlopen\n",
        "\n",
        "\n",
        "def flatten_structs(nested_df):\n",
        "    stack = [((), nested_df)]\n",
        "    columns = []\n",
        "\n",
        "    while len(stack) > 0:\n",
        "\n",
        "        parents, df = stack.pop()\n",
        "\n",
        "        array_cols = [\n",
        "            c[0]\n",
        "            for c in df.dtypes\n",
        "            if c[1][:5] == \"array\"\n",
        "        ]\n",
        "\n",
        "        flat_cols = [\n",
        "            f.col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\n",
        "            for c in df.dtypes\n",
        "            if c[1][:6] != \"struct\"\n",
        "        ]\n",
        "\n",
        "        nested_cols = [\n",
        "            c[0]\n",
        "            for c in df.dtypes\n",
        "            if c[1][:6] == \"struct\"\n",
        "        ]\n",
        "\n",
        "        columns.extend(flat_cols)\n",
        "\n",
        "        for nested_col in nested_cols:\n",
        "            projected_df = df.select(nested_col + \".*\")\n",
        "            stack.append((parents + (nested_col,), projected_df))\n",
        "\n",
        "    return nested_df.select(columns)\n",
        "\n",
        "def flatten_array_struct_df(df):\n",
        "\n",
        "    array_cols = [\n",
        "            c[0]\n",
        "            for c in df.dtypes\n",
        "            if c[1][:5] == \"array\"\n",
        "        ]\n",
        "\n",
        "\n",
        "    while len(array_cols) > 0:\n",
        "\n",
        "        for array_col in array_cols:\n",
        "            # check if array contains structure then flatten\n",
        "            if isinstance(df.schema[array_col].dataType.elementType, StructType) is True:\n",
        "\n",
        "                cols_to_select = [x for x in df.columns if x != array_col ]\n",
        "\n",
        "                df = df.withColumn(array_col, f.explode(f.col(array_col)))\n",
        "\n",
        "        df = flatten_structs(df)\n",
        "\n",
        "        array_cols = [\n",
        "            c[0]\n",
        "            for c in df.dtypes\n",
        "            if c[1][:5] == \"array\"\n",
        "        ]\n",
        "        return df\n",
        "\n",
        "\n",
        "url = 'https://world.openfoodfacts.org/api/v2/product/737628064502.json'\n",
        "jsonData = urlopen(url).read().decode('utf-8')\n",
        "rdd = spark.sparkContext.parallelize([jsonData])\n",
        "df_link = spark.read.json(rdd, multiLine=True)\n",
        "\n",
        "#load json object\n",
        "df = spark.read.option(\"multiline\",\"true\").json('/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/data.json')\n",
        "\n",
        "df_link_fl = flatten_structs(df)\n",
        "df_link_fl.show()\n",
        "df_link_fl = flatten_array_struct_df(df_link_fl)\n",
        "df_link_fl.show()"
      ],
      "metadata": {
        "id": "pLWXXNVCCxK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf87d1ba-e589-4dfe-9a74-890c3a57b640"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----+--------------------+-------------+----+--------------------+-----+--------------------+\n",
            "|              ____id|  id|               items|         name| ppu|             topping| type|      batters_batter|\n",
            "+--------------------+----+--------------------+-------------+----+--------------------+-----+--------------------+\n",
            "|[1, 2, 3, 4, 8888...|0001|                NULL|         Cake|0.55|[{5001, None}, {5...|donut|[{1001, Regular},...|\n",
            "|                NULL|0002|                NULL|       Raised|0.55|[{5001, None}, {5...|donut|   [{1001, Regular}]|\n",
            "|                NULL|0003|[{\"m2yskBQFythfE4...|Old Fashioned|0.55|[{5001, None}, {5...|donut|[{1001, Regular},...|\n",
            "+--------------------+----+--------------------+-------------+----+--------------------+-----+--------------------+\n",
            "\n",
            "+------+----+-------------+----+-----+-----------------+-------------------+----------+------------+--------------------+--------+--------------------+------------------------+-----------------------+-------------------+\n",
            "|____id|  id|         name| ppu| type|batters_batter_id|batters_batter_type|topping_id|topping_type|          items_etag|items_id|          items_kind|items_snippet_assignable|items_snippet_channelId|items_snippet_title|\n",
            "+------+----+-------------+----+-----+-----------------+-------------------+----------+------------+--------------------+--------+--------------------+------------------------+-----------------------+-------------------+\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1001|            Regular|      5001|        None|\"m2yskBQFythfE4ir...|       1|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Film & Animation|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1002|          Chocolate|      5001|        None|\"m2yskBQFythfE4ir...|       1|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Film & Animation|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1001|            Regular|      5002|      Glazed|\"m2yskBQFythfE4ir...|       1|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Film & Animation|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1002|          Chocolate|      5002|      Glazed|\"m2yskBQFythfE4ir...|       1|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Film & Animation|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1001|            Regular|      5003|   Chocolate|\"m2yskBQFythfE4ir...|       1|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Film & Animation|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1002|          Chocolate|      5003|   Chocolate|\"m2yskBQFythfE4ir...|       1|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Film & Animation|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1001|            Regular|      5004|       Maple|\"m2yskBQFythfE4ir...|       1|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Film & Animation|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1002|          Chocolate|      5004|       Maple|\"m2yskBQFythfE4ir...|       1|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Film & Animation|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1001|            Regular|      5001|        None|\"m2yskBQFythfE4ir...|       2|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Autos & Vehicles|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1002|          Chocolate|      5001|        None|\"m2yskBQFythfE4ir...|       2|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Autos & Vehicles|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1001|            Regular|      5002|      Glazed|\"m2yskBQFythfE4ir...|       2|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Autos & Vehicles|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1002|          Chocolate|      5002|      Glazed|\"m2yskBQFythfE4ir...|       2|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Autos & Vehicles|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1001|            Regular|      5003|   Chocolate|\"m2yskBQFythfE4ir...|       2|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Autos & Vehicles|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1002|          Chocolate|      5003|   Chocolate|\"m2yskBQFythfE4ir...|       2|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Autos & Vehicles|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1001|            Regular|      5004|       Maple|\"m2yskBQFythfE4ir...|       2|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Autos & Vehicles|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1002|          Chocolate|      5004|       Maple|\"m2yskBQFythfE4ir...|       2|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|   Autos & Vehicles|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1001|            Regular|      5001|        None|\"m2yskBQFythfE4ir...|      10|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|              Music|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1002|          Chocolate|      5001|        None|\"m2yskBQFythfE4ir...|      10|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|              Music|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1001|            Regular|      5002|      Glazed|\"m2yskBQFythfE4ir...|      10|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|              Music|\n",
            "|  NULL|0003|Old Fashioned|0.55|donut|             1002|          Chocolate|      5002|      Glazed|\"m2yskBQFythfE4ir...|      10|youtube#videoCate...|                    true|   UCBR8-60-B28hp2Bm...|              Music|\n",
            "+------+----+-------------+----+-----+-----------------+-------------------+----------+------------+--------------------+--------+--------------------+------------------------+-----------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ofmp4Vn18T0-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}