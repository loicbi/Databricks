{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "420d7f19-aa8d-4f24-8755-536c131156a1",
     "showTitle": false,
     "title": ""
    },
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/loicbi/Databricks/blob/develop/19_Databricks_%26_Pyspark__Real_Time_ETL_Pipeline_Azure_SQL_to_ADLS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d69a6f-1748-4474-87a3-48a2b95934fb",
     "showTitle": false,
     "title": ""
    },
    "id": "-E3fsmF-x6H4"
   },
   "outputs": [],
   "source": [
    "storage_account = '***'\n",
    "access_key = '***='\n",
    "container_name = '***'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b808b26d-c764-4019-88b7-d4d6e8b36ec9",
     "showTitle": false,
     "title": ""
    },
    "id": "1jZYQQo4x6H6"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    f'fs.azure.account.key.{storage_account}.dfs.core.windows.net',\n",
    "    access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cee63d92-da31-47c7-aaa6-97bc1f51dc77",
     "showTitle": false,
     "title": ""
    },
    "id": "DImeEYQBx6H6"
   },
   "source": [
    "Step 1: Extract Data from ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4bc1615-3ad9-40f5-a59a-8f3452d41a9e",
     "showTitle": false,
     "title": ""
    },
    "id": "MK4iZmfIx6H7"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  explode\n",
    "\n",
    "# set files location ADLS\n",
    "files_location = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/api=state/\"\n",
    "\n",
    "df_state_json = spark.read.json(files_location)\n",
    "# df_state_json.select('*', explode('*')).show()\n",
    "df_state_json.printSchema()\n",
    "display(df_state_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7251e2-7978-453d-92c7-71dac3770f32",
     "showTitle": false,
     "title": ""
    },
    "id": "x1j1cNlox6H8"
   },
   "source": [
    "Step 2: Show data json state and explode from ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c75eef9-12b9-4778-b983-ffc29e060f0c",
     "showTitle": false,
     "title": ""
    },
    "id": "jVo4sPhYx6H8"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  explode\n",
    "import json\n",
    "# set files location ADLS\n",
    "files_location = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/__ALF__/\"\n",
    "df_product_json = spark.read.option('multiline', 'true').json(files_location)\n",
    "# df_product_json.printSchema()\n",
    "# display(df_product_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf177079-f233-413b-a6aa-0c5e9db2b16a",
     "showTitle": false,
     "title": ""
    },
    "id": "ZjEVjTnqx6H8"
   },
   "source": [
    "Step 3: Load transformed Data into ADLS\n",
    "Create Mount Point for ADLS Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b10f86-a210-45f7-9f44-2ba9f095879d",
     "showTitle": false,
     "title": ""
    },
    "id": "Y8KMB2yGx6H9"
   },
   "outputs": [],
   "source": [
    "configs = {f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\": access_key}\n",
    "\n",
    "\n",
    "dbutils.fs.mount(\n",
    "source = f\"wasbs://{container_name}@{storage_account}.blob.core.windows.net/__ALF__\",\n",
    "mount_point = \"/mnt/adls_test\",\n",
    "extra_configs = configs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d215be0e-2409-467b-a171-3744937a6b13",
     "showTitle": false,
     "title": ""
    },
    "id": "pT-LLtA_x6H9"
   },
   "source": [
    "Write the Data in Parquet Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c79e04e-5433-4bf1-8014-7a10cc7efabf",
     "showTitle": false,
     "title": ""
    },
    "id": "orL6BoAEx6H9"
   },
   "outputs": [],
   "source": [
    "df_product_json.write.format('parquet').mode('overwrite').save('/mnt/adls_test/adv_work_json/')\n",
    "dbutils.fs.refreshMounts()\n",
    "dbutils.fs.ls(\"/mnt/adls_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57f2f8f6-810d-4e64-a183-a2af914842e9",
     "showTitle": false,
     "title": ""
    },
    "id": "CLEABRezx6H9"
   },
   "source": [
    "Write the Data in CSV Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7dac8e2-db46-4fbb-acca-631715194896",
     "showTitle": false,
     "title": ""
    },
    "id": "gz_1Volrx6H9"
   },
   "outputs": [],
   "source": [
    "df_product_json.write.format('csv').mode('overwrite').option(\"header\", \"true\").save('/mnt/adls_test/adv_work_csv/')\n",
    "dbutils.fs.refreshMounts()\n",
    "# IF THERE IS A PROBLEM BECAUSE MY FILE IS A JSON FORMAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "010b569a-c799-44e3-9716-8d7b1c1f90e7",
     "showTitle": false,
     "title": ""
    },
    "id": "C_i5JJJux6H-"
   },
   "source": [
    "Flatten data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a28615-c76c-4ec6-bff1-f49a16372a5b",
     "showTitle": false,
     "title": ""
    },
    "id": "uM9cfjXCx6H-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "\n",
    "def flatten_structs(nested_df):\n",
    "    stack = [((), nested_df)]\n",
    "    columns = []\n",
    "\n",
    "    while len(stack) > 0:\n",
    "\n",
    "        parents, df = stack.pop()\n",
    "\n",
    "        array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "\n",
    "        flat_cols = [\n",
    "            f.col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\n",
    "            for c in df.dtypes\n",
    "            if c[1][:6] != \"struct\"\n",
    "        ]\n",
    "\n",
    "        nested_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:6] == \"struct\"\n",
    "        ]\n",
    "\n",
    "        columns.extend(flat_cols)\n",
    "\n",
    "        for nested_col in nested_cols:\n",
    "            projected_df = df.select(nested_col + \".*\")\n",
    "            stack.append((parents + (nested_col,), projected_df))\n",
    "\n",
    "    return nested_df.select(columns)\n",
    "\n",
    "def flatten_array_struct_df(df):\n",
    "\n",
    "    array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    while len(array_cols) > 0:\n",
    "\n",
    "        for array_col in array_cols:\n",
    "            # check if array contains structure then flatten\n",
    "            if isinstance(df.schema[array_col].dataType.elementType, StructType) is True:\n",
    "\n",
    "                cols_to_select = [x for x in df.columns if x != array_col ]\n",
    "\n",
    "                df = df.withColumn(array_col, f.explode(f.col(array_col)))\n",
    "\n",
    "        df = flatten_structs(df)\n",
    "\n",
    "        array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "        return df\n",
    "\n",
    "\n",
    "# get columns list from dataFrame\n",
    "list_column_ims = list(df_product_json.columns)\n",
    "\n",
    "\n",
    "\n",
    "# Display the column types\n",
    "def get_column_type_from_df (df) -> list:\n",
    "    # Get the column types\n",
    "    column_types = df.dtypes\n",
    "    # for column_name, data_type in column_types:\n",
    "    #     print(f\"Column '{column_name}' has data type: {data_type}\")\n",
    "    return [data_type for data_type in column_types]\n",
    "\n",
    "\n",
    "\n",
    "for col in list_column_ims:\n",
    "    df_col = df_product_json.select(col)\n",
    "\n",
    "    list_type = get_column_type_from_df(df_col)\n",
    "\n",
    "    if col == 'identification':\n",
    "        df_col = flatten_structs(df_col)\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "\n",
    "    if col == 'marketingContent':\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "    if col == 'planoContent':\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "    if col == 'newItemSetup':\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "    if col == 'ecommerceContent':\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "        df_col = flatten_array_struct_df(df_col)\n",
    "    display(df_col)\n",
    "\n",
    "\n",
    "# df_fl = flatten_array_struct_df(df_product_json)\n",
    "# display(df_fl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6bf2d314-3078-49d1-87ef-22db44b19254",
     "showTitle": false,
     "title": ""
    },
    "id": "uTGmvDI-x6H-"
   },
   "source": [
    "Delete File if finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5957f3-fa0a-481f-895f-30735c1c1b3b",
     "showTitle": false,
     "title": ""
    },
    "id": "-SWh7eXxx6H-"
   },
   "outputs": [],
   "source": [
    "\n",
    "# dELETE the DBFS root\n",
    "dbutils.fs.unmount(\"/mnt/adls_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f992a654-c822-4bc0-9d9e-03eda618b7bf",
     "showTitle": false,
     "title": ""
    },
    "id": "2qnGIWjcx6H-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "19_Databricks_&_Pyspark__Real_Time_ETL_Pipeline_Azure_SQL_to_ADLS",
   "widgets": {}
  },
  "colab": {
   "include_colab_link": true,
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
