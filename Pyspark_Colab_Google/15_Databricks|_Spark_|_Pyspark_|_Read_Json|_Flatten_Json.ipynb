{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fe228b2-e976-40a9-affd-6c2a6ccfbec9",
     "showTitle": false,
     "title": ""
    },
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/loicbi/Databricks/blob/develop/15_Databricks%7C_Spark_%7C_Pyspark_%7C_Read_Json%7C_Flatten_Json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32433acb-a64d-462c-b940-f86d199a3389",
     "showTitle": false,
     "title": ""
    },
    "id": "5poKsdENgL1a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83d1ccb0-1b3f-4320-af29-5ae9a3fe421a",
     "showTitle": false,
     "title": ""
    },
    "id": "PAHeaUwSIS1Y"
   },
   "outputs": [],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d9ec429-3122-4bb4-b837-b1477a098418",
     "showTitle": false,
     "title": ""
    },
    "id": "q3N_gZyPFhMw"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aecc1710-b3ca-403e-a870-cf17296118b6",
     "showTitle": false,
     "title": ""
    },
    "id": "2YNmbGsDFon9"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql.functions import  * # to_date, col, split, char_length, lit, trim\n",
    "# findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# spark = SparkSession.builder.appName(\"app_name\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\n",
    "df = spark.read.json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/US_category_id.json\", multiLine=True)\n",
    "# df = spark.read.json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/product.json\", multiLine=True)\n",
    "df.printSchema()\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c4f7dac-903f-4a76-b275-9038528eda0d",
     "showTitle": false,
     "title": ""
    },
    "id": "AQhewVetmaG9"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Flatten array of structs and structs\n",
    "def flatten_df(df):\n",
    "  #  compute Complex Fields (Lists and Structs) in Schema\n",
    "   complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   while len(complex_fields)!=0:\n",
    "      col_name=list(complex_fields.keys())[0]\n",
    "      print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
    "\n",
    "      # if StructType then convert all sub element to columns.\n",
    "      # i.e. flatten structs\n",
    "      if (type(complex_fields[col_name]) == StructType):\n",
    "         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
    "         df=df.select(\"*\", *expanded).drop(col_name)\n",
    "\n",
    "      # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "      # i.e. explode Arrays\n",
    "      elif (type(complex_fields[col_name]) == ArrayType):\n",
    "         df=df.withColumn(col_name,explode_outer(col_name))\n",
    "\n",
    "      # recompute remaining Complex Fields in Schema\n",
    "      complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47bbf369-0ac6-4c73-87db-fde2fe446812",
     "showTitle": false,
     "title": ""
    },
    "id": "pYv1Tt87mpFF"
   },
   "outputs": [],
   "source": [
    "df_flatten = flatten_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eff78c1-0d80-4a95-a3cc-eeb6ea1a44c8",
     "showTitle": false,
     "title": ""
    },
    "id": "Qu741ttJqSOd"
   },
   "outputs": [],
   "source": [
    "df_flatten.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "065f22b5-a65c-46e2-a198-461d6d3762e6",
     "showTitle": false,
     "title": ""
    },
    "id": "Ow6igr1bn-VG"
   },
   "source": [
    "## json from link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4288b9-8df7-4820-bf3e-b4cf51287d68",
     "showTitle": false,
     "title": ""
    },
    "id": "E3_9m_tAn9VX"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import findspark\n",
    "import json\n",
    "from pyspark.sql.functions import  * # to_date, col, split, char_length, lit, trim\n",
    "# findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# spark = SparkSession.builder.appName(\"app_name\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\n",
    "#Flatten array of structs and structs\n",
    "def flatten_df_all(df):\n",
    "   i_column_exist = 0\n",
    "  #  compute Complex Fields (Lists and Structs) in Schema\n",
    "   complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   while len(complex_fields)!=0:\n",
    "      col_name=list(complex_fields.keys())[0]\n",
    "      # print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
    "\n",
    "      # if StructType then convert all sub element to columns.\n",
    "      # i.e. flatten structs\n",
    "      if (type(complex_fields[col_name]) == StructType):\n",
    "         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
    "         df=df.select(\"*\", *expanded).drop(col_name)\n",
    "\n",
    "      # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "      # i.e. explode Arrays\n",
    "      elif (type(complex_fields[col_name]) == ArrayType):\n",
    "          # check if colNameList [] contains StructType ==> [{}]\n",
    "          print(isinstance(df.schema[col_name].dataType.elementType, StructType))\n",
    "          df.select(col_name).show()\n",
    "          # if isinstance(df.schema[col_name].dataType.elementType, StructType) is True:\n",
    "          if col_name in df.columns:\n",
    "            df=df.withColumnRenamed(col_name,col_name+'_'+str(i_column_exist)).withColumn(col_name+'_'+str(i_column_exist),explode_outer(col_name+'_'+str(i_column_exist)))\n",
    "          else:\n",
    "            df=df.withColumn(col_name,explode_outer(col_name))\n",
    "\n",
    "\n",
    "      # recompute remaining Complex Fields in Schema\n",
    "      complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   i_column_exist += 1\n",
    "   return df\n",
    "\n",
    "# method from link\n",
    "# url = 'https://world.openfoodfacts.org/api/v2/product/737628064502.json'\n",
    "# jsonData = urlopen(url).read().decode('utf-8')\n",
    "# rdd = spark.sparkContext.parallelize([jsonData])\n",
    "# df_link = spark.read.json(rdd, multiLine=True)\n",
    "\n",
    "# method from drive\n",
    "df_product_2 = spark.read.json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/product2.json\", multiLine=True)\n",
    "\n",
    "# Create outer method to return the flattened Data Frame\n",
    "def flatten_json_df(_df: DataFrame) -> DataFrame:\n",
    "    # List to hold the dynamically generated column names\n",
    "    flattened_col_list = []\n",
    "\n",
    "    # Inner method to iterate over Data Frame to generate the column list\n",
    "    def get_flattened_cols(df: DataFrame, struct_col: str = None) -> None:\n",
    "        for col in df.columns:\n",
    "            # print(col, df.schema[col].dataType.typeName(), df.schema[col].dataType)\n",
    "\n",
    "            if df.schema[col].dataType.typeName() != 'struct':\n",
    "                if (isinstance(df.schema[col].dataType, ArrayType)):\n",
    "                  if (isinstance(df.schema[col].dataType.elementType, StructType)):\n",
    "                    array_struct_col = col\n",
    "                    # print(col, 'struct_col:', struct_col)\n",
    "\n",
    "                    for col_name in (json.loads(df.schema[col].dataType.json())['elementType']['fields']):\n",
    "                        # for c in json.loads(df.schema[col].dataType.json())['elementType']['fields'][n]['name']:\n",
    "                        # print(col_name['name'])\n",
    "                        col_child = col_name['name']\n",
    "                        array_chained_col = array_struct_col +\".\"+ col_child if array_struct_col is not None else col\n",
    "                        # get_flattened_cols(df.select(col_child+\".*\"), col)\n",
    "                        print(array_chained_col)\n",
    "\n",
    "\n",
    "\n",
    "                        # array_t = struct_col + \".\" + col\n",
    "                        # flattened_col_list.append(f\"{t} as {t.replace('.','_')}\")\n",
    "\n",
    "\n",
    "\n",
    "                    # if ('type' in (json.loads(df.schema[col].dataType.json()))):\n",
    "                    #   print(col, 'contains structure')\n",
    "                    # else:\n",
    "                    #   print(col, 'doesn\\'t contains structure')\n",
    "\n",
    "\n",
    "\n",
    "                if struct_col is None:\n",
    "                    flattened_col_list.append(f\"{col} as {col.replace('.','_')}\")\n",
    "                else:\n",
    "                    t = struct_col + \".\" + col\n",
    "                    flattened_col_list.append(f\"{t} as {t.replace('.','_')}\")\n",
    "                    print('not struc', t)\n",
    "            else:\n",
    "                chained_col = struct_col +\".\"+ col if struct_col is not None else col\n",
    "                print('struct', chained_col)\n",
    "                get_flattened_cols(df.select(col+\".*\"), chained_col)\n",
    "\n",
    "    # Call the inner Method\n",
    "    get_flattened_cols(_df)\n",
    "\n",
    "    # Return the flattened Data Frame\n",
    "    return _df.selectExpr(flattened_col_list)# Generate the flattened DF\n",
    "\n",
    "# df.printSchema()\n",
    "df_link_flatten = flatten_json_df(df_product_2)\n",
    "df_link_flatten.show()\n",
    "# df_link = spark.read.json('https://world.openfoodfacts.org/api/v2/product/737628064502.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12c42b4d-e56f-4b9f-99c7-e13b6cc4a891",
     "showTitle": false,
     "title": ""
    },
    "id": "3dUg6JAEqUOt"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "data = r'''\n",
    "{\n",
    " \"kind\": \"youtube#videoCategoryListResponse\",\n",
    " \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/S730Ilt-Fi-emsQJvJAAShlR6hM\\\"\",\n",
    " \"items\": [\n",
    "  {\n",
    "   \"kind\": \"youtube#videoCategory\",\n",
    "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/Xy1mB4_yLrHy_BmKmPBggty2mZQ\\\"\",\n",
    "   \"id\": \"1\",\n",
    "   \"snippet\": {\n",
    "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
    "    \"title\": \"Film & Animation\",\n",
    "    \"assignable\": true\n",
    "   }\n",
    "  },\n",
    "  {\n",
    "   \"kind\": \"youtube#videoCategory\",\n",
    "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/UZ1oLIIz2dxIhO45ZTFR3a3NyTA\\\"\",\n",
    "   \"id\": \"2\",\n",
    "   \"snippet\": {\n",
    "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
    "    \"title\": \"Autos & Vehicles\",\n",
    "    \"assignable\": true\n",
    "   }\n",
    "  },\n",
    "  {\n",
    "   \"kind\": \"youtube#videoCategory\",\n",
    "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/nqRIq97-xe5XRZTxbknKFVe5Lmg\\\"\",\n",
    "   \"id\": \"10\",\n",
    "   \"snippet\": {\n",
    "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
    "    \"title\": \"Music\",\n",
    "    \"assignable\": true\n",
    "   }\n",
    "  },\n",
    "  {\n",
    "   \"kind\": \"youtube#videoCategory\",\n",
    "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/HwXKamM1Q20q9BN-oBJavSGkfDI\\\"\",\n",
    "   \"id\": \"15\",\n",
    "   \"snippet\": {\n",
    "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
    "    \"title\": \"Pets & Animals\",\n",
    "    \"assignable\": true\n",
    "   }\n",
    "  },\n",
    "  {\n",
    "   \"kind\": \"youtube#videoCategory\",\n",
    "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/9GQMSRjrZdHeb1OEM1XVQ9zbGec\\\"\",\n",
    "   \"id\": \"17\",\n",
    "   \"snippet\": {\n",
    "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
    "    \"title\": \"Sports\",\n",
    "    \"assignable\": true\n",
    "   }\n",
    "  },\n",
    "  {\n",
    "   \"kind\": \"youtube#videoCategory\",\n",
    "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/FJwVpGCVZ1yiJrqZbpqe68Sy_OE\\\"\",\n",
    "   \"id\": \"18\",\n",
    "   \"snippet\": {\n",
    "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
    "    \"title\": \"Short Movies\",\n",
    "    \"assignable\": false\n",
    "   }\n",
    "  },\n",
    "  {\n",
    "   \"kind\": \"youtube#videoCategory\",\n",
    "   \"etag\": \"\\\"m2yskBQFythfE4irbTIeOgYYfBU/M-3iD9dwK7YJCafRf_DkLN8CouA\\\"\",\n",
    "   \"id\": \"19\",\n",
    "   \"snippet\": {\n",
    "    \"channelId\": \"UCBR8-60-B28hp2BmDPdntcQ\",\n",
    "    \"title\": \"Travel & Events\",\n",
    "    \"assignable\": true\n",
    "   }\n",
    "  }\n",
    " ]\n",
    "} '''\n",
    "data = json.loads(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00c45e71-5a23-4a10-9312-b54770e82711",
     "showTitle": false,
     "title": ""
    },
    "id": "q2EBMvkP6Vsc"
   },
   "source": [
    "## PySpark — Flatten JSON/Struct Data Frame dynamically\n",
    "### Example variable local\n",
    "\n",
    "[JSON/Struct Data Frame dynamically](https://subhamkharwal.medium.com/pyspark-flatten-json-struct-data-frame-dynamically-c2e5d8937dcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "227d84b2-2b86-43eb-af92-90f694e1d8f9",
     "showTitle": false,
     "title": ""
    },
    "id": "jQjICq4g4_G_"
   },
   "outputs": [],
   "source": [
    "# Lets create an Example Data Frame to hold JSON data\n",
    "# Example Data Frame with column having JSON data\n",
    "_data = [\n",
    "    ['EMP001', '{\"dept\" : \"account\", \"fname\": \"Ramesh\", \"lname\": \"Singh\", \"skills\": [\"excel\", \"tally\", \"word\"]}'],\n",
    "    ['EMP002', '{\"dept\" : \"sales\", \"fname\": \"Siv\", \"lname\": \"Kumar\", \"skills\": [\"biking\", \"sales\"]}'],\n",
    "    ['EMP003', '{\"dept\" : \"hr\", \"fname\": \"MS Raghvan\", \"skills\": [\"communication\", \"soft-skills\"], \"hobbies\" : {\"cycling\": \"expert\", \"computers\":\"basic\"}}'],\n",
    "    ['EMP003', '{\"dept\" : \"hr\", \"fname\": \"MS Raghvan\", \"skills\": [\"communication\", \"soft-skills\"], \"hobbies\" : {\"cycling\": \"expert\", \"computers\":\"basic\"}}']\n",
    "]\n",
    "\n",
    "# Columns for the data\n",
    "_cols = ['emp_no', 'raw_data']\n",
    "\n",
    "# Lets create the raw Data Frame\n",
    "df_raw = spark.createDataFrame(data = _data, schema = _cols)\n",
    "\n",
    "# Determine the schema of the JSON payload from the column\n",
    "json_schema_df = spark.read.json(df_raw.rdd.map(lambda row: row.raw_data))\n",
    "json_schema = json_schema_df.schema\n",
    "\n",
    "# Apply the schema to payload to read the data\n",
    "from pyspark.sql.functions import from_json\n",
    "df_details = df_raw.withColumn(\"emp_details\", from_json(df_raw[\"raw_data\"], json_schema)).drop(\"raw_data\")\n",
    "df_details.show(10, False)\n",
    "\n",
    "df_details.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ac55f68-c08d-4b04-b296-d1ce1cee4bb2",
     "showTitle": false,
     "title": ""
    },
    "id": "7X0IutCLHTKG"
   },
   "outputs": [],
   "source": [
    "# Python function to flatten the data dynamically\n",
    "from pyspark.sql import DataFrame\n",
    "# Create outer method to return the flattened Data Frame\n",
    "def flatten_json_df(_df: DataFrame) -> DataFrame:\n",
    "    # List to hold the dynamically generated column names\n",
    "    flattened_col_list = []\n",
    "\n",
    "    # Inner method to iterate over Data Frame to generate the column list\n",
    "    def get_flattened_cols(df: DataFrame, struct_col: str = None) -> None:\n",
    "        for col in df.columns:\n",
    "            if df.schema[col].dataType.typeName() != 'struct':\n",
    "                if struct_col is None:\n",
    "                    flattened_col_list.append(f\"{col} as {col.replace('.','_')}\")\n",
    "                else:\n",
    "                    t = struct_col + \".\" + col\n",
    "                    flattened_col_list.append(f\"{t} as {t.replace('.','_')}\")\n",
    "            else:\n",
    "                chained_col = struct_col +\".\"+ col if struct_col is not None else col\n",
    "                get_flattened_cols(df.select(col+\".*\"), chained_col)\n",
    "\n",
    "    # Call the inner Method\n",
    "    get_flattened_cols(_df)\n",
    "\n",
    "    # Return the flattened Data Frame\n",
    "    return _df.selectExpr(flattened_col_list)# Generate the flattened DF\n",
    "flattened_df = flatten_json_df(df_details)\n",
    "flattened_df.show(10)\n",
    "# Print Schema\n",
    "flattened_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fc49d7c-a5b2-4f2c-a462-7f14b81d125c",
     "showTitle": false,
     "title": ""
    },
    "id": "BAOJHPQTFXb_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = '''\n",
    "{\"customer_id\":45721,\"firstname\":\"Tiffany\",\"lastname\":\"Skinner\",\"salutation\":\"Miss\",\"gender\":\"F\",\"birthdate\":\"1935-09-13\",\"birth_country\":\"RWANDA\",\"address_id\":2133,\"email_address\":\"Tiffany.Skinner@J.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[50001,60000],\"buy_potential\":\">10000\",\"purchase_estimate\":1000,\"vehicle_count\":2,\"credit_rating\":\"High Risk\",\"education_status\":\"Secondary\"}}\n",
    "{\"customer_id\":72411,\"firstname\":\"Dennis\",\"lastname\":\"Pettit\",\"salutation\":\"Mr.\",\"gender\":\"F\",\"birthdate\":\"1973-11-09\",\"birth_country\":\"MALAWI\",\"address_id\":25398,\"email_address\":\"Dennis.Pettit@PtS7Fbdax.com\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[170001,180000],\"buy_potential\":\"0-500\",\"purchase_estimate\":3000,\"vehicle_count\":4,\"credit_rating\":\"Unknown\",\"education_status\":\"Primary\"}}\n",
    "{\"customer_id\":50358,\"firstname\":\"Issac\",\"lastname\":\"Saunders\",\"salutation\":\"Mr.\",\"gender\":\"F\",\"birthdate\":\"1979-03-30\",\"birth_country\":\"JAMAICA\",\"address_id\":23242,\"email_address\":\"Issac.Saunders@Ykkz.edu\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[120001,130000],\"buy_potential\":\"0-500\",\"purchase_estimate\":2500,\"vehicle_count\":3,\"credit_rating\":\"Unknown\",\"education_status\":\"Unknown\"}}\n",
    "{\"customer_id\":96143,\"firstname\":\"Carl\",\"lastname\":\"Mitchell\",\"salutation\":\"Dr.\",\"gender\":\"F\",\"birthdate\":\"1966-10-16\",\"birth_country\":\"IRELAND\",\"address_id\":37550,\"email_address\":\"Carl.Mitchell@iqios5dGzoAkgSfdkO.org\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[20001,30000],\"buy_potential\":\"Unknown\",\"purchase_estimate\":1000,\"vehicle_count\":-1,\"credit_rating\":\"High Risk\",\"education_status\":\"College\"}}\n",
    "{\"customer_id\":93545,\"firstname\":\"Antonio\",\"lastname\":\"Lyon\",\"salutation\":\"Dr.\",\"gender\":\"F\",\"birthdate\":\"1953-05-24\",\"birth_country\":\"PAPUA NEW GUINEA\",\"address_id\":22369,\"email_address\":\"Antonio.Lyon@Alqrvh.org\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[180001,190000],\"buy_potential\":\"1001-5000\",\"purchase_estimate\":8000,\"vehicle_count\":1,\"credit_rating\":\"Unknown\",\"education_status\":\"4 yr Degree\"}}\n",
    "{\"customer_id\":92534,\"firstname\":\"Jesse\",\"lastname\":\"Alexander\",\"salutation\":\"Dr.\",\"gender\":\"M\",\"birthdate\":\"1968-10-22\",\"birth_country\":\"ZIMBABWE\",\"address_id\":7628,\"email_address\":\"Jesse.Alexander@OU.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[120001,130000],\"buy_potential\":\">10000\",\"purchase_estimate\":8000,\"vehicle_count\":2,\"credit_rating\":\"Unknown\",\"education_status\":\"Advanced Degree\"}}\n",
    "{\"customer_id\":91798,\"firstname\":\"Philip\",\"lastname\":\"Miller\",\"salutation\":\"Mr.\",\"gender\":\"M\",\"birthdate\":\"1934-08-05\",\"birth_country\":\"DJIBOUTI\",\"address_id\":24321,\"email_address\":\"Philip.Miller@1j1uLUoeT3N.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[150001,160000],\"buy_potential\":\"5001-10000\",\"purchase_estimate\":500,\"vehicle_count\":3,\"credit_rating\":\"High Risk\",\"education_status\":\"Unknown\"}}\n",
    "{\"customer_id\":82953,\"firstname\":\"Ronald\",\"lastname\":\"Harris\",\"salutation\":\"Sir\",\"gender\":\"M\",\"birthdate\":\"1958-08-02\",\"birth_country\":\"LESOTHO\",\"address_id\":49705,\"email_address\":\"Ronald.Harris@KUjp3Gqoqt.com\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[20001,30000],\"buy_potential\":\"501-1000\",\"purchase_estimate\":6000,\"vehicle_count\":4,\"credit_rating\":\"Good\",\"education_status\":\"4 yr Degree\"}}\n",
    "{\"customer_id\":70926,\"firstname\":\"Myrtle\",\"lastname\":\"Wolford\",\"salutation\":\"Dr.\",\"gender\":\"F\",\"birthdate\":\"1983-11-14\",\"birth_country\":\"C�TE D'IVOIRE\",\"address_id\":36381,\"email_address\":\"Myrtle.Wolford@lbKOsj.com\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[110001,120000],\"buy_potential\":\"Unknown\",\"purchase_estimate\":2500,\"vehicle_count\":0,\"credit_rating\":\"Unknown\",\"education_status\":\"2 yr Degree\"}}\n",
    "{\"customer_id\":12157,\"firstname\":\"William\",\"lastname\":\"Daniel\",\"salutation\":\"Mr.\",\"gender\":\"F\",\"birthdate\":\"1976-03-27\",\"birth_country\":\"BANGLADESH\",\"address_id\":40974,\"email_address\":\"William.Daniel@Ri4LeFs5lB4QpUfF.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[170001,180000],\"buy_potential\":\">10000\",\"purchase_estimate\":6000,\"vehicle_count\":2,\"credit_rating\":\"Unknown\",\"education_status\":\"Primary\"}}\n",
    "{\"customer_id\":65983,\"firstname\":\"Perla\",\"lastname\":\"Edwards\",\"salutation\":\"Dr.\",\"gender\":\"F\",\"birthdate\":\"1947-02-27\",\"birth_country\":\"LIBERIA\",\"address_id\":3282,\"email_address\":\"Perla.Edwards@nvR7A3h9o.edu\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[180001,190000],\"buy_potential\":\"0-500\",\"purchase_estimate\":9500,\"vehicle_count\":1,\"credit_rating\":\"Good\",\"education_status\":\"Advanced Degree\"}}\n",
    "{\"customer_id\":36493,\"firstname\":\"Jeffrey\",\"lastname\":\"Clary\",\"salutation\":\"Mr.\",\"gender\":\"F\",\"birthdate\":\"1924-02-17\",\"birth_country\":\"ROMANIA\",\"address_id\":1289,\"email_address\":\"Jeffrey.Clary@lBvUIdcnhNFey.com\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[40001,50000],\"buy_potential\":\"0-500\",\"purchase_estimate\":2500,\"vehicle_count\":1,\"credit_rating\":\"Low Risk\",\"education_status\":\"2 yr Degree\"}}\n",
    "{\"customer_id\":41148,\"firstname\":\"Charles\",\"lastname\":\"Patterson\",\"salutation\":\"Mr.\",\"gender\":\"M\",\"birthdate\":\"1961-08-12\",\"birth_country\":\"DOMINICA\",\"address_id\":29846,\"email_address\":\"Charles.Patterson@bHiA5JcaZzpS89L8iL.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[120001,130000],\"buy_potential\":\"0-500\",\"purchase_estimate\":10000,\"vehicle_count\":1,\"credit_rating\":\"Unknown\",\"education_status\":\"Advanced Degree\"}}\n",
    "{\"customer_id\":61969,\"firstname\":\"Nikki\",\"lastname\":\"Watson\",\"salutation\":\"Miss\",\"gender\":\"M\",\"birthdate\":\"1977-08-15\",\"birth_country\":\"PARAGUAY\",\"address_id\":22881,\"email_address\":\"Nikki.Watson@DNC.org\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[90001,100000],\"buy_potential\":\"1001-5000\",\"purchase_estimate\":6500,\"vehicle_count\":0,\"credit_rating\":\"High Risk\",\"education_status\":\"Advanced Degree\"}}\n",
    "{\"customer_id\":69480,\"firstname\":\"Marion\",\"lastname\":\"Paul\",\"salutation\":\"Sir\",\"gender\":\"M\",\"birthdate\":\"1955-10-11\",\"birth_country\":\"SAUDI ARABIA\",\"address_id\":42838,\"email_address\":\"Marion.Paul@tfyhY8prgDTSdZ.org\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[100001,110000],\"buy_potential\":\"5001-10000\",\"purchase_estimate\":3000,\"vehicle_count\":0,\"credit_rating\":\"Unknown\",\"education_status\":\"Unknown\"}}\n",
    "{\"customer_id\":4739,\"firstname\":\"Gregory\",\"lastname\":\"Martin\",\"salutation\":\"Mr.\",\"gender\":\"M\",\"birthdate\":\"1958-03-15\",\"birth_country\":\"UGANDA\",\"address_id\":2625,\"email_address\":\"Gregory.Martin@GI78ZP8x.edu\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[130001,140000],\"buy_potential\":\"1001-5000\",\"purchase_estimate\":5500,\"vehicle_count\":4,\"credit_rating\":\"Good\",\"education_status\":\"Unknown\"}}\n",
    "{\"customer_id\":67634,\"salutation\":\"Sir\",\"gender\":\"F\",\"address_id\":37149,\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[140001,150000],\"buy_potential\":\"Unknown\",\"purchase_estimate\":9000,\"vehicle_count\":1,\"credit_rating\":\"High Risk\",\"education_status\":\"Primary\"}}\n",
    "{\"customer_id\":87143,\"firstname\":\"Corazon\",\"lastname\":\"Martinez\",\"salutation\":\"Miss\",\"gender\":\"F\",\"birthdate\":\"1984-08-13\",\"birth_country\":\"SAMOA\",\"address_id\":27621,\"email_address\":\"Corazon.Martinez@BrDtDnXnkci1Pp2an.edu\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[30001,40000],\"buy_potential\":\"5001-10000\",\"purchase_estimate\":9000,\"vehicle_count\":4,\"credit_rating\":\"High Risk\",\"education_status\":\"Unknown\"}}\n",
    "{\"customer_id\":64173,\"firstname\":\"Christopher\",\"lastname\":\"Byrd\",\"salutation\":\"Sir\",\"gender\":\"M\",\"birthdate\":\"1936-04-10\",\"birth_country\":\"MALDIVES\",\"address_id\":21565,\"email_address\":\"Christopher.Byrd@bLMxmInbyn.org\",\"is_preffered_customer\":\"Y\",\"demographics\":{\"income_range\":[20001,30000],\"buy_potential\":\"0-500\",\"purchase_estimate\":8500,\"vehicle_count\":4,\"credit_rating\":\"Good\",\"education_status\":\"2 yr Degree\"}}\n",
    "{\"customer_id\":39412,\"firstname\":\"Constance\",\"lastname\":\"Seaman\",\"salutation\":\"Miss\",\"gender\":\"F\",\"birthdate\":\"1980-03-10\",\"birth_country\":\"AUSTRALIA\",\"address_id\":18881,\"email_address\":\"Constance.Seaman@GItsENvPOs7C.org\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[40001,50000],\"buy_potential\":\"5001-10000\",\"purchase_estimate\":3000,\"vehicle_count\":2,\"credit_rating\":\"High Risk\",\"education_status\":\"4 yr Degree\"}}\n",
    "{\"customer_id\":39412,\"firstname\":\"Constance\",\"lastname\":\"Seaman\",\"salutation\":\"Miss\",\"gender\":\"F\",\"birthdate\":\"1980-03-10\",\"birth_country\":\"AUSTRALIA\",\"address_id\":18881,\"email_address\":\"Constance.Seaman@GItsENvPOs7C.org\",\"is_preffered_customer\":\"N\",\"demographics\":{\"income_range\":[40001,50000],\"buy_potential\":\"5001-10000\",\"purchase_estimate\":3000,\"vehicle_count\":2,\"credit_rating\":\"High Risk\",\"education_status\":\"4 yr Degree\"}}\n",
    "'''\n",
    "\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def flatten_json(json_obj, prefix=''):\n",
    "    flattened = {}\n",
    "    for key, value in json_obj.items():\n",
    "        new_key = prefix + key\n",
    "        if isinstance(value, dict):\n",
    "            flattened.update(flatten_json(value, new_key + '_'))\n",
    "        elif isinstance(value, list):\n",
    "            for i, item in enumerate(value):\n",
    "                if isinstance(item, dict):\n",
    "                    flattened.update(flatten_json(item, new_key + f'_{i}_'))\n",
    "                else:\n",
    "                    flattened[new_key + f'_{i}'] = item\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load the data as separate JSON objects\n",
    "json_objects = [json.loads(line) for line in data.strip().split('\\n')]\n",
    "\n",
    "# Flatten the JSON objects\n",
    "flattened_objects = [flatten_json(obj) for obj in json_objects]\n",
    "\n",
    "# Convert the flattened objects into a Spark DataFrame\n",
    "df = spark.createDataFrame(flattened_objects)\n",
    "df.printSchema()\n",
    "# Show the flattened DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "313ac835-f21a-4707-884e-4a34a70a11ea",
     "showTitle": false,
     "title": ""
    },
    "id": "qcz0z46RQ2Rp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cda514d-6d75-4d3e-93b8-99a4ad1cd0ed",
     "showTitle": false,
     "title": ""
    },
    "id": "Sh7xOEWhBxA-"
   },
   "outputs": [],
   "source": [
    "df_customer =  spark.read.json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/customer.json\")\n",
    "df_customer.printSchema()\n",
    "# df_customer.show()\n",
    "# Flatten the JSON objects\n",
    "# flattened_objects = [flatten_json(obj) for obj in df_customer]\n",
    "\n",
    "# # Convert the flattened objects into a Spark DataFrame\n",
    "df = flatten_df(df_customer)\n",
    "\n",
    "df.orderBy(df.customer_id).show()\n",
    "df.printSchema()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fc30424-8747-4aeb-9b61-d24c9a4f2047",
     "showTitle": false,
     "title": ""
    },
    "id": "Re1G5IcHAPAF"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "705c5d02-0998-4021-985b-2f6b9d96333b",
     "showTitle": false,
     "title": ""
    },
    "id": "Vekd5Y6-Ns_5"
   },
   "source": [
    "## Flattening deep nested JSON using python and Databricks\n",
    "[link text](https://sanajitghosh.medium.com/flattening-deep-nested-json-using-python-and-databricks-110d097efa69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91a54d8b-7555-4baf-a145-ab3b72c1eee7",
     "showTitle": false,
     "title": ""
    },
    "id": "lQqEIDYiNscF"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import arrays_zip\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "#load json object\n",
    "df = spark.read.option(\"multiline\",\"true\").json('/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/data.json')\n",
    "\n",
    "def child_struct(nested_df):\n",
    "    # Creating python list to store dataframe metadata\n",
    "    list_schema = [((), nested_df)]\n",
    "    # Creating empty python list for final flattern columns\n",
    "    flat_columns = []\n",
    "\n",
    "    while len(list_schema) > 0:\n",
    "      # Removing latest or recently added item (dataframe schema) and returning into df variable\n",
    "          parents, df = list_schema.pop()\n",
    "          flat_cols = [  col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],))) for c in df.dtypes if c[1][:6] != \"struct\"   ]\n",
    "\n",
    "          struct_cols = [  c[0]   for c in df.dtypes if c[1][:6] == \"struct\"   ]\n",
    "\n",
    "          flat_columns.extend(flat_cols)\n",
    "          #Reading  nested columns and appending into stack list\n",
    "          for i in struct_cols:\n",
    "                projected_df = df.select(i + \".*\")\n",
    "                list_schema.append((parents + (i,), projected_df))\n",
    "    return nested_df.select(flat_columns)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "def master_array(df):\n",
    "    array_cols = [c[0] for c in df.dtypes if c[1][:5]==\"array\"]\n",
    "    while len(array_cols)>0:\n",
    "        # array\n",
    "        for c in array_cols:\n",
    "           df = df.withColumn(c,explode_outer(c))\n",
    "        df = child_struct(df)\n",
    "        array_cols = [c[0] for c in df.dtypes if c[1][:5]==\"array\"]\n",
    "    return df\n",
    "\n",
    "df_output = master_array(df)\n",
    "display(df_output)\n",
    "df_output.show(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68321d4a-b5f9-40bb-a20f-b41f37671ca3",
     "showTitle": false,
     "title": ""
    },
    "id": "4brJ8hec382b"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "def flatten_structs(nested_df):\n",
    "    stack = [((), nested_df)]\n",
    "    columns = []\n",
    "\n",
    "    while len(stack) > 0:\n",
    "\n",
    "        parents, df = stack.pop()\n",
    "\n",
    "        array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "\n",
    "        flat_cols = [\n",
    "            f.col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\n",
    "            for c in df.dtypes\n",
    "            if c[1][:6] != \"struct\"\n",
    "        ]\n",
    "\n",
    "        nested_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:6] == \"struct\"\n",
    "        ]\n",
    "\n",
    "        columns.extend(flat_cols)\n",
    "\n",
    "        for nested_col in nested_cols:\n",
    "            projected_df = df.select(nested_col + \".*\")\n",
    "            stack.append((parents + (nested_col,), projected_df))\n",
    "\n",
    "    return nested_df.select(columns)\n",
    "\n",
    "def flatten_array_struct_df(df):\n",
    "\n",
    "    array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    while len(array_cols) > 0:\n",
    "\n",
    "        for array_col in array_cols:\n",
    "            if isinstance(df.schema[array_col].dataType.elementType, StructType) is True:\n",
    "\n",
    "                cols_to_select = [x for x in df.columns if x != array_col ]\n",
    "\n",
    "                df = df.withColumn(array_col, f.explode(f.col(array_col)))\n",
    "\n",
    "        df = flatten_structs(df)\n",
    "\n",
    "        array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "        return df\n",
    "\n",
    "flat_df = flatten_array_struct_df(df_product_2)\n",
    "\n",
    "flat_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d564d558-f27c-455c-9a13-539b35cfa532",
     "showTitle": false,
     "title": ""
    },
    "id": "yztcvZaZ7HM6"
   },
   "source": [
    "## David et Moi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89c84601-e426-496b-acc7-deab77e7e014",
     "showTitle": false,
     "title": ""
    },
    "id": "pLWXXNVCCxK-"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "def flatten_structs(nested_df):\n",
    "    stack = [((), nested_df)]\n",
    "    columns = []\n",
    "\n",
    "    while len(stack) > 0:\n",
    "\n",
    "        parents, df = stack.pop()\n",
    "\n",
    "        array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "\n",
    "        flat_cols = [\n",
    "            f.col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\n",
    "            for c in df.dtypes\n",
    "            if c[1][:6] != \"struct\"\n",
    "        ]\n",
    "\n",
    "        nested_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:6] == \"struct\"\n",
    "        ]\n",
    "\n",
    "        columns.extend(flat_cols)\n",
    "\n",
    "        for nested_col in nested_cols:\n",
    "            projected_df = df.select(nested_col + \".*\")\n",
    "            stack.append((parents + (nested_col,), projected_df))\n",
    "\n",
    "    return nested_df.select(columns)\n",
    "\n",
    "def flatten_array_struct_df(df):\n",
    "\n",
    "    array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    while len(array_cols) > 0:\n",
    "\n",
    "        for array_col in array_cols:\n",
    "            # check if array contains structure then flatten\n",
    "            if isinstance(df.schema[array_col].dataType.elementType, StructType) is True:\n",
    "\n",
    "                cols_to_select = [x for x in df.columns if x != array_col ]\n",
    "\n",
    "                df = df.withColumn(array_col, f.explode(f.col(array_col)))\n",
    "\n",
    "        df = flatten_structs(df)\n",
    "\n",
    "        array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "        return df\n",
    "\n",
    "\n",
    "url = 'https://world.openfoodfacts.org/api/v2/product/737628064502.json'\n",
    "jsonData = urlopen(url).read().decode('utf-8')\n",
    "rdd = spark.sparkContext.parallelize([jsonData])\n",
    "df_link = spark.read.json(rdd, multiLine=True)\n",
    "\n",
    "#load json object\n",
    "df = spark.read.option(\"multiline\",\"true\").json('/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/data.json')\n",
    "\n",
    "df_link_fl = flatten_structs(df)\n",
    "df_link_fl.show()\n",
    "df_link_fl = flatten_array_struct_df(df_link_fl)\n",
    "df_link_fl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a48b79a9-d3cd-4420-95a1-c10323db5e53",
     "showTitle": false,
     "title": ""
    },
    "id": "ofmp4Vn18T0-"
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as f\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "def flatten_structs(nested_df):\n",
    "    stack = [((), nested_df)]\n",
    "    columns = []\n",
    "\n",
    "    while len(stack) > 0:\n",
    "\n",
    "        parents, df = stack.pop()\n",
    "\n",
    "        array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "\n",
    "        flat_cols = [\n",
    "            f.col(\".\".join(parents + (c[0],))).alias(\"_\".join(parents + (c[0],)))\n",
    "            for c in df.dtypes\n",
    "            if c[1][:6] != \"struct\"\n",
    "        ]\n",
    "\n",
    "        nested_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:6] == \"struct\"\n",
    "        ]\n",
    "\n",
    "        columns.extend(flat_cols)\n",
    "\n",
    "        for nested_col in nested_cols:\n",
    "            projected_df = df.select(nested_col + \".*\")\n",
    "            stack.append((parents + (nested_col,), projected_df))\n",
    "\n",
    "    return nested_df.select(columns)\n",
    "\n",
    "def flatten_array_struct_df(df):\n",
    "\n",
    "    array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    while len(array_cols) > 0:\n",
    "\n",
    "        for array_col in array_cols:\n",
    "            # check if array contains structure then flatten\n",
    "            if isinstance(df.schema[array_col].dataType.elementType, StructType) is True:\n",
    "\n",
    "                cols_to_select = [x for x in df.columns if x != array_col ]\n",
    "\n",
    "                df = df.withColumn(array_col, f.explode(f.col(array_col)))\n",
    "\n",
    "        df = flatten_structs(df)\n",
    "\n",
    "        array_cols = [\n",
    "            c[0]\n",
    "            for c in df.dtypes\n",
    "            if c[1][:5] == \"array\"\n",
    "        ]\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "df = spark.read.json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/R_0Bd3pzvYZ7Jy0KZ.json\", multiLine=True)\n",
    "# df = spark.read.json(\"/content/drive/MyDrive/DATA_ENGINEER/DATABRICKS/AAAA_________IAM___CODING_____Colab Notebooks/Learning/datasset/AFCON/json_files/qualtrics.json\", multiLine=True)\n",
    "# df.printSchema()\n",
    "# df.show(10)\n",
    "\n",
    "df_link_fl = flatten_structs(df)\n",
    "df_link_fl.show()\n",
    "df_link_fl = flatten_array_struct_df(df_link_fl)\n",
    "df_link_fl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ba904c0-925a-4437-96b8-95b5c81b724a",
     "showTitle": false,
     "title": ""
    },
    "id": "g2IuY0ey_s0g"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "15_Databricks|_Spark_|_Pyspark_|_Read_Json|_Flatten_Json",
   "widgets": {}
  },
  "colab": {
   "authorship_tag": "ABX9TyPGECU3F/m+2QWkJplohfXL",
   "include_colab_link": true,
   "mount_file_id": "1rOlnpxy_p3aDFuHKpjRlBI7h9M-NwkqV",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
