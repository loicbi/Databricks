{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "413bd811-e44a-4292-a654-f6e72e0182fa",
     "showTitle": false,
     "title": ""
    },
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/loicbi/Databricks/blob/develop/17_Databricks_%26_Pyspark__Azure_Data_Lake_Storage_Integration_with_Databricks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fde399c-15ba-4026-a543-73ae35f2d22d",
     "showTitle": false,
     "title": ""
    },
    "id": "Fx77Myf-dGjN"
   },
   "source": [
    "## **Methods of ADLS Integration Databricks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73df6451-0e8d-4caa-8d80-1283f5e2e751",
     "showTitle": false,
     "title": ""
    },
    "id": "2YNmbGsDFon9"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "# findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# spark = SparkSession.builder.appName(\"app_name\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d69a6f-1748-4474-87a3-48a2b95934fb",
     "showTitle": false,
     "title": ""
    },
    "id": "4SKiOSc3dGjO"
   },
   "outputs": [],
   "source": [
    "storage_account = '*****'\n",
    "access_key = '******************'\n",
    "container_name = '*****'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c123c974-1531-4543-9f63-972a5b5bbea9",
     "showTitle": false,
     "title": ""
    },
    "id": "583mr2nndGjP"
   },
   "source": [
    "#### Method 1: Using DLS access key directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa96cec-872a-41aa-a304-557290d57095",
     "showTitle": false,
     "title": ""
    },
    "id": "07fLD7bpdGjQ"
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\n",
    "    f'fs.azure.account.key.{storage_account}.dfs.core.windows.net',\n",
    "    access_key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d442956d-daaf-4730-a77d-37a249b48a1a",
     "showTitle": false,
     "title": ""
    },
    "id": "TCBkUXmkdGjQ"
   },
   "source": [
    "##### ls command (dbutils.fs.ls) from data lake azure\n",
    "https://learn.microsoft.com/en-us/azure/databricks/dev-tools/databricks-utils#--file-system-utility-dbutilsfs\n",
    "\n",
    "Lists the contents of a directory.\n",
    "\n",
    "To display help for this command, run dbutils.fs.help(\"ls\").\n",
    "\n",
    "This example displays information about the contents of /tmp. The modificationTime field is available in Databricks Runtime 10.2 and above. In R, modificationTime is returned as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56f3a535-78b3-4f82-b771-70218ccc1cb4",
     "showTitle": false,
     "title": ""
    },
    "id": "idEZ2K7TdGjQ"
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/api=state/date=2024-04-11\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d7251e2-7978-453d-92c7-71dac3770f32",
     "showTitle": false,
     "title": ""
    },
    "id": "Cjk66A-7dGjQ"
   },
   "source": [
    "Show data json state and explode from ADLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c75eef9-12b9-4778-b983-ffc29e060f0c",
     "showTitle": false,
     "title": ""
    },
    "id": "9uxD-0SvdGjQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import  explode\n",
    "\n",
    "# set files location ADLS\n",
    "files_location = f\"abfss://{container_name}@{storage_account}.dfs.core.windows.net/api=state/\"\n",
    "\n",
    "df_state_json = spark.read.json(files_location)\n",
    "# df_state_json.select('*', explode('*')).show()\n",
    "df_state_json.printSchema()\n",
    "display(df_state_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e109fd49-866c-4d49-b40b-c401712d66c1",
     "showTitle": false,
     "title": ""
    },
    "id": "TBDFFC2jdGjR"
   },
   "source": [
    "#### Method 2: Mount your storage account to your Databricks cluster\n",
    "\n",
    "dbutils.fs.mount\n",
    "\n",
    "In this section, you mount your Azure Data Lake Storage Gen2 cloud object storage to the Databricks File System (DBFS). You use the Azure AD service principle you created previously for authentication with the storage account. For more information, see Mounting cloud object storage on Azure Databricks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f2c56d-7621-48ee-8639-24759d4d2147",
     "showTitle": false,
     "title": ""
    },
    "id": "pL4U6mtgdGjR"
   },
   "outputs": [],
   "source": [
    "configs = {f\"fs.azure.account.key.{storage_account}.blob.core.windows.net\": access_key}\n",
    "\n",
    "dbutils.fs.mount(\n",
    "source = f\"wasbs://{container_name}@{storage_account}.blob.core.windows.net/api=state/\",\n",
    "mount_point = \"/mnt/adls_test\",\n",
    "extra_configs = configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c81f560-a430-4c09-904a-b5cd89a75d69",
     "showTitle": false,
     "title": ""
    },
    "id": "DGZ4Uui0dGjR"
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/mnt/adls_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed773e94-98ff-4874-aee3-022c2b2d956b",
     "showTitle": false,
     "title": ""
    },
    "id": "jBNIoLsVdGjR"
   },
   "source": [
    "### refreshMounts command (dbutils.fs.refreshMounts)\n",
    "Forces all machines in the cluster to refresh their mount cache, ensuring they receive the most recent information.\n",
    "\n",
    "To display help for this command, run dbutils.fs.help(\"refreshMounts\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21663538-0015-41cf-a261-93ef2a62b67d",
     "showTitle": false,
     "title": ""
    },
    "id": "mGgzS-2KdGjR"
   },
   "outputs": [],
   "source": [
    "dbutils.fs.refreshMounts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31eded92-87c1-4156-a2fc-2bcf0ec409d7",
     "showTitle": false,
     "title": ""
    },
    "id": "CsUZP5V_dGjR"
   },
   "source": [
    "### unmount command (dbutils.fs.unmount)\n",
    "Deletes a DBFS mount point.\n",
    "\n",
    " Warning\n",
    "\n",
    "To avoid errors, never modify a mount point while other jobs are reading or writing to it. After modifying a mount, always run dbutils.fs.refreshMounts() on all other running clusters to propagate any mount updates. See refreshMounts command (dbutils.fs.refreshMounts).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "168ab6de-e0b6-41c6-a411-73bfef2c3dcb",
     "showTitle": false,
     "title": ""
    },
    "id": "9XLIt_DhdGjR"
   },
   "outputs": [],
   "source": [
    "dbutils.fs.unmount(\"/mnt/adls_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d47d1bbf-4b0b-413e-92b6-13b6c57837d8",
     "showTitle": false,
     "title": ""
    },
    "id": "ntgOHw38dGjR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "17_Databricks_&_Pyspark__Azure_Data_Lake_Storage_Integration_with_Databricks",
   "widgets": {}
  },
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
